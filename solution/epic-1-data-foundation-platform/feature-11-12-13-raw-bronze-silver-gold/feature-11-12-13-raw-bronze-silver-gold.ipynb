{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3a3122-04cb-4562-b4a6-e7c5f81e5e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Epic 1 â€“ Data Foundation Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96a790bf-0f78-4f9a-b26a-8e069d092aeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature 1.1: Raw Data Ingestion (Sprint 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG & INITIALIZATION\n",
    "# Widgets allow parameterization when running as a Job\n",
    "import re, pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Unity Catalog catalog name (leave blank if not using UC)\n",
    "dbutils.widgets.text(\"catalog\", \"\", \"Unity Catalog (optional)\")\n",
    "# Optional prefix to distinguish multiple student/dev environments\n",
    "dbutils.widgets.text(\"env_prefix\", \"\", \"Env prefix (short)\")\n",
    "# Raw brand list (comma separated)\n",
    "dbutils.widgets.text(\"brands\", \"contoso,eurostyle\", \"Brands CSV list\")\n",
    "# Overwrite mode flag (true/false) for idempotent reruns\n",
    "dbutils.widgets.text(\"full_reload\", \"false\", \"Full reload?\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\").strip()\n",
    "env_prefix = dbutils.widgets.get(\"env_prefix\").strip()\n",
    "full_reload = dbutils.widgets.get(\"full_reload\").lower() == \"true\"\n",
    "brands = [b.strip().lower() for b in dbutils.widgets.get(\"brands\").split(\",\") if b.strip()]\n",
    "\n",
    "# Schemas (databases) expected for minimal path (no UC: hive_metastore)\n",
    "SCHEMAS = [\"raw\", \"bronze\", \"silver\", \"gold\", \"monitor\", \"ref\"]\n",
    "\n",
    "# Helper to qualify (catalog.schema.table) if catalog provided\n",
    "def fq(schema: str, table: str) -> str:\n",
    "    if catalog:\n",
    "        return f\"{catalog}.{schema}.{table}\"\n",
    "    return f\"{schema}.{table}\"\n",
    "\n",
    "# Create schemas if they do not exist (works with or without UC)\n",
    "for s in SCHEMAS:\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog + '.' if catalog else ''}{s}\")\n",
    "\n",
    "# Paths (DBFS). In later hardening you could externalize to ABFSS mounts.\n",
    "RAW_BASE = \"/FileStore/retail/raw\"\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"auto\")\n",
    "\n",
    "print(f\"Catalog: {catalog or 'N/A (hive_metastore)'}  | Env prefix: {env_prefix or 'none'}  | Full reload: {full_reload}\")\n",
    "print(f\"Brands: {brands}\")\n",
    "\n",
    "# Generic CSV reader with light schema inference (can be replaced with explicit schema)\n",
    "def load_raw_csv(path: str) -> DataFrame:\n",
    "    df = (spark.read\n",
    "          .option(\"header\", True)\n",
    "          .option(\"inferSchema\", True)\n",
    "          .csv(path))\n",
    "    return df\n",
    "\n",
    "# Add lineage columns & standardize column names (snake_case)\n",
    "def standardize(df: DataFrame, source_system: str) -> DataFrame:\n",
    "    # Normalize column names\n",
    "    renamed = df\n",
    "    for c in df.columns:\n",
    "        snake = re.sub(r\"[^a-zA-Z0-9]+\", \"_\", c.strip()).lower().strip(\"_\")\n",
    "        if snake != c:\n",
    "            renamed = renamed.withColumnRenamed(c, snake)\n",
    "    enriched = (renamed\n",
    "                .withColumn(\"ingest_ts\", F.current_timestamp())\n",
    "                .withColumn(\"source_system\", F.lit(source_system.upper())))\n",
    "    return enriched\n",
    "\n",
    "# Idempotent write (overwrite entire table or MERGE on set of keys)\n",
    "def write_delta(df: DataFrame, schema: str, table: str, keys: list[str] | None = None):\n",
    "    target = fq(schema, table)\n",
    "    if not spark._jsparkSession.catalog().tableExists(target):\n",
    "        (df.write\n",
    "           .format(\"delta\")\n",
    "           .mode(\"overwrite\")\n",
    "           .option(\"overwriteSchema\", \"true\")\n",
    "           .saveAsTable(target))\n",
    "        print(f\"Created table {target}\")\n",
    "        return\n",
    "    if full_reload or not keys:\n",
    "        (df.write\n",
    "           .format(\"delta\")\n",
    "           .mode(\"overwrite\")\n",
    "           .option(\"overwriteSchema\", \"true\")\n",
    "           .saveAsTable(target))\n",
    "        print(f\"Overwrote table {target}\")\n",
    "    else:\n",
    "        # MERGE logic\n",
    "        dt = DeltaTable.forName(spark, target)\n",
    "        alias_src = \"src\"\n",
    "        alias_tgt = \"tgt\"\n",
    "        cond = \" AND \".join([f\"{alias_src}.{k} <=> {alias_tgt}.{k}\" for k in keys])\n",
    "        update_set = {c: f\"{alias_src}.{c}\" for c in df.columns}\n",
    "        (dt.alias(alias_tgt)\n",
    "           .merge(df.alias(alias_src), cond)\n",
    "           .whenMatchedUpdate(set=update_set)\n",
    "           .whenNotMatchedInsert(values=update_set)\n",
    "           .execute())\n",
    "        print(f\"Merged into {target} on keys {keys}\")\n",
    "\n",
    "# Simple utility for row count persistence\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {fq('monitor','dq_bronze_daily')} (\n",
    "  run_date DATE,\n",
    "  brand STRING,\n",
    "  table_name STRING,\n",
    "  row_count BIGINT,\n",
    "  ingestion_ts TIMESTAMP\n",
    ") USING delta PARTITIONED BY (run_date)\n",
    "\"\"\")\n",
    "print(\"Initialized monitoring table dq_bronze_daily (if not exists).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d26094f-eae5-4c5b-9e68-86b5dd0136d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1) ðŸŸ¥ Create raw landing folders in DBFS (`/FileStore/retail/raw/contoso/`, `/FileStore/retail/raw/eurostyle/`) and document paths in the runbook.  \n",
    "[DBX-DE-Assoc][Medallion][Platform]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure raw brand folders exist in DBFS\n",
    "for b in brands:\n",
    "    path = f\"{RAW_BASE}/{b}\"\n",
    "    dbutils.fs.mkdirs(path)\n",
    "    print(f\"Ensured {path}\")\n",
    "\n",
    "display(dbutils.fs.ls(RAW_BASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e2a521c-34f6-4a33-af7b-5d67bc759d1d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"created_by\":272,\"credential_name\":257,\"url\":400},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757278196967}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE EXTERNAL LOCATION IF NOT EXISTS loc_raw\n",
    "  URL 'abfss://raw@stescontosoma.dfs.core.windows.net/'\n",
    "  WITH (STORAGE CREDENTIAL `ws_es_contoso_ma`);\n",
    "\n",
    "DESCRIBE EXTERNAL LOCATION loc_raw;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc509f5-bf0e-47b8-875f-bf75763d585e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create directories in DBFS\n",
    "dbutils.fs.mkdirs(\"/FileStore/retail/raw/contoso\")\n",
    "dbutils.fs.mkdirs(\"/FileStore/retail/raw/eurostyle\")\n",
    "\n",
    "# Verify that they exist\n",
    "display(dbutils.fs.ls(\"/FileStore/retail/raw\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6028718c-a59b-4c4a-8fe4-965b4769ce72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. ðŸŸ¥ Upload Contoso CSVs to the raw path; note file names, counts, and approximate sizes.\n",
    "[DBX-DE-Assoc][Medallion] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "139dc876-60a1-4bab-af0b-83a51cd6da2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca3ae7d2-274d-4a7f-be67-27c41bd37533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. ðŸŸ¥ Ingest Contoso to Delta Bronze with lineage columns (ingest_ts, source_system='CONTOSO') as bronze.sales_contoso\n",
    "[DBX-DE-Assoc][Delta-Basics][Autoloader][CopyInto][Medallion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccd98a87-b025-446e-beff-61fde47fb239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Ingest Contoso raw CSV(s) to bronze\n",
    "contoso_path = f\"{RAW_BASE}/contoso\"\n",
    "# Example: assume files uploaded manually (UI) to contoso_path\n",
    "contoso_df = load_raw_csv(contoso_path)\n",
    "contoso_std = standardize(contoso_df, \"CONTOSO\")\n",
    "print(f\"Loaded Contoso rows: {contoso_std.count()}\")\n",
    "\n",
    "# Example business keys (adjust if schema differs)\n",
    "contoso_keys = [c for c in [\"order_id\", \"sku\", \"customer_id\", \"order_date\"] if c in contoso_std.columns]\n",
    "write_delta(contoso_std, \"bronze\", \"sales_contoso\", keys=contoso_keys)\n",
    "\n",
    "spark.sql(f\"COMMENT ON TABLE {fq('bronze','sales_contoso')} IS 'Raw ingested Contoso sales with lineage columns.'\")\n",
    "\n",
    "display(spark.table(fq('bronze','sales_contoso')).limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b9db60-2c60-454a-94ae-afbb3564834f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. ðŸŸ¥ Create a BI-friendly Contoso view bronze.v_sales_contoso with trimmed/typed columns for Power BI DirectQuery.\n",
    "[DBX-DA-Assoc][SQL-Basics][Dashboards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2083cabd-adec-41a3-b5c3-c6f316de8712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Create BI view (trim/typed) for DirectQuery\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {fq('bronze','v_sales_contoso')} AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  sku,\n",
    "  CAST(order_date AS DATE) AS order_date,\n",
    "  TRY_CAST(total_amount AS DOUBLE) AS total_amount,\n",
    "  currency,\n",
    "  source_system,\n",
    "  ingest_ts\n",
    "FROM {fq('bronze','sales_contoso')}\n",
    "\"\"\")\n",
    "print(\"Created/Updated view bronze.v_sales_contoso\")\n",
    "\n",
    "display(spark.sql(f\"SELECT * FROM {fq('bronze','v_sales_contoso')} LIMIT 10\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff2b41ea-c03a-49ca-ae4b-829811002663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. ðŸŸ¥ Register tables/views in the metastore (Unity Catalog or workspace) and add table comments.\n",
    "[DBX-DE-Assoc][UC-Permissions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4e31657-f6f9-48a9-afac-7328a3eed1ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: (Optional) Additional comments / metadata tagging\n",
    "spark.sql(f\"COMMENT ON VIEW {fq('bronze','v_sales_contoso')} IS 'BI-friendly Contoso sales view for DirectQuery (trimmed & typed).'\")\n",
    "\n",
    "print(\"Tables & Views in bronze schema:\")\n",
    "bronze_items = spark.sql(f\"SHOW TABLES IN {(catalog + '.bronze') if catalog else 'bronze'}\")\n",
    "display(bronze_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a4012b3-9801-45ce-8cb9-3181ace43043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. ðŸŸ¥ Validate Contoso types (dates/numerics), address corrupt records if any, and record issues.\n",
    "[DBX-DE-Assoc][Delta-Basics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4190ce36-f17e-4d33-9161-127aed88bec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Validate types & basic quality\n",
    "from pyspark.sql.functions import col, count, sum as Fsum\n",
    "bronze_contoso = spark.table(fq('bronze','sales_contoso'))\n",
    "\n",
    "row_count = bronze_contoso.count()\n",
    "null_counts = bronze_contoso.select([Fsum(col(c).isNull().cast('int')).alias(c) for c in bronze_contoso.columns])\n",
    "print(f\"Row count: {row_count}\")\n",
    "print(\"Null counts per column:\")\n",
    "display(null_counts)\n",
    "\n",
    "# Simple date sanity check if order_date present\n",
    "if 'order_date' in bronze_contoso.columns:\n",
    "    date_min, date_max = bronze_contoso.select(F.min('order_date'), F.max('order_date')).first()\n",
    "    print(f\"Date range: {date_min} -> {date_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5b1e249-2cd7-4b94-9265-4548ce2fdfb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. ðŸŸ¨ Perform a Power BI DirectQuery smoke test to bronze.v_sales_contoso; capture steps/screenshot in the README.\n",
    "[DBX-DA-Assoc][Dashboards][MS-PL300][Visualize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd2cbc0a-a590-4f5f-bb34-8a8b94189aa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d41efc2-b7fa-4eb5-96f5-44c911518da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8) ðŸŸ¥ Upload EuroStyle CSVs to the raw path and capture source metadata (provenance, obtained date). \n",
    "[DBX-DE-Assoc][Medallion]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5d4a1ec-4b5f-4837-b5c2-c557787afea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9) ðŸŸ¥ Ingest EuroStyle to Delta Bronze with lineage columns (`ingest_ts`, `source_system='EUROSTYLE'`) as `bronze.sales_eurostyle`.  \n",
    "[DBX-DE-Assoc][Delta-Basics][Autoloader][CopyInto][Medallion] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "376b7885-a553-41ae-8ca8-e0f6218e6371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7a8a0c5-f6a6-4d02-a77b-c81494dedc49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 10) ðŸŸ¥ Create and check in `docs/column_mapping.csv` with `source_name, unified_name, target_type`.  \n",
    "[DBX-DE-Prof][Modeling]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65321c81-49b1-4b1b-a1bd-89311860f64d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 11) ðŸŸ¥ Apply initial schema alignment across brands using the mapping and naming conventions (snake_case, consistent date/decimal types); update the runbook.  \n",
    "[DBX-DE-Prof][Modeling]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d95e89f-faa7-4ea6-9e3f-421df0438dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2cd1f52-920a-4eac-a78d-d156a96c482d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 12) ðŸŸ¥ Reconcile rawâ†’Bronze row counts per brand (Â±1% tolerance or explained variance) and persist counts to `monitor.dq_bronze_daily`.  \n",
    "[DBX-DE-Prof][Monitoring-Logs]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9eb2352-192c-40ae-9e5a-88efcaaa787d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8a23736-53ec-4abf-b399-833a1d7e3931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 13) ðŸŸ¥ Compute a basic DQ summary: null rates on keys, duplicate rate on `(order_id, sku, customer_id, order_date)`, top countries/currencies; publish a one-pager.  \n",
    "[DBX-DE-Prof][Monitoring-Logs]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3ce768a-db56-4494-b326-e50b9d1b8163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c16c4f65-7f32-4321-8238-a8f1928e04c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 14) ðŸŸ¥ Enforce basic Delta constraints where feasible (NOT NULL on business keys, simple CHECKs); record violations.  \n",
    "[DBX-DE-Assoc][Delta-Basics]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f7e6cba-c0c6-4d81-89be-8ad31851c60f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a1dd9e0-0bac-42d6-ac6b-580a2e3b3695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 15) ðŸŸ¥ [DBX-DE-Assoc][Delta-MERGE][Delta-Basics][Medallion]  \n",
    "Implement an idempotent re-run strategy (deterministic overwrite by date window via `replaceWhere` or `MERGE` on business keys) and verify repeatability.  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6122718319245312,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "feature-11-12-13-raw-bronze-silver-gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
